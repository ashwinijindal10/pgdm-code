{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement a L-layer deep model on MNIST dataset using Keras. The MNIST dataset contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. MNIST's name comes from the fact that it is a modified subset of two data sets collected by NIST, the United States' National Institute of Standards and Technology.<br>\n",
    "<br>\n",
    "<br>\n",
    "To use Keras, you'll need to install Keras and Tensorflow.\n",
    "<br>\n",
    "Please run the following commands if you don't have Keras and TensorFlow already installed.\n",
    "<br>\n",
    "1. ! pip install TensorFlow\n",
    "<br>\n",
    "2. ! pip install keras\n",
    "<br>\n",
    "3. ! pip install msgpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras import regularizers\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset we use here is 'mnist.pkl.gz' which is divided into training, validation and test data. The following function <i> load_data() </i> unpacks the file and extracts the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    f.seek(0)\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature dataset is:[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "The target dataset is:[5 0 4 ... 8 4 8]\n",
      "The number of examples in the training dataset is:50000\n",
      "The number of points in a single input is:784\n"
     ]
    }
   ],
   "source": [
    "print(\"The feature dataset is:\" + str(training_data[0]))\n",
    "print(\"The target dataset is:\" + str(training_data[1]))\n",
    "print(\"The number of examples in the training dataset is:\" + str(len(training_data[0])))\n",
    "print(\"The number of points in a single input is:\" + str(len(training_data[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as discussed earlier in the lectures, the target variable is converted to a one hot matrix. We use the function <i> one_hot </i> to convert the target dataset to one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(j):\n",
    "    # input is the target dataset of shape (1, m) where m is the number of data points\n",
    "    # returns a 2 dimensional array of shape (10, m) where each target value is converted to a one hot encoding\n",
    "    # Look at the next block of code for a better understanding of one hot encoding\n",
    "    n = j.shape[0]\n",
    "    new_array = np.zeros((10, n))\n",
    "    index = 0\n",
    "    for res in j:\n",
    "        new_array[res][index] = 1\n",
    "        index = index + 1\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(np.array([1,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "one_hot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = np.array(tr_d[0][:]).T\n",
    "    training_results = np.array(tr_d[1][:])\n",
    "    train_set_y = one_hot(training_results)\n",
    "    \n",
    "    validation_inputs = np.array(va_d[0][:]).T\n",
    "    validation_results = np.array(va_d[1][:])\n",
    "    validation_set_y = one_hot(validation_results)\n",
    "    \n",
    "    test_inputs = np.array(te_d[0][:]).T\n",
    "    test_results = np.array(te_d[1][:])\n",
    "    test_set_y = one_hot(test_results)\n",
    "    \n",
    "    return (training_inputs, train_set_y, validation_inputs, validation_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For implementing in Keras, the input training and input target dataset are supposed to have shape (m, n) where m is the number of training samples and n is the number of parts in a single input.\n",
    "<br> Hence, let create the desired dataset shapes by taking transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x.T\n",
    "train_set_y = train_set_y.T\n",
    "test_set_x = test_set_x.T\n",
    "test_set_y = test_set_y.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if the datasets are in the desired shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (50000, 784)\n",
      "train_set_y shape: (50000, 10)\n",
      "test_set_x shape: (10000, 784)\n",
      "test_set_y shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualise the dataset. Feel free to change the index to see if the training data has been correctly tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf137c4548>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index  = 1006\n",
    "k = train_set_x[index,:]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label= training_data[1][index]))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a framework. So, to implement a neural network model in Keras, we first create an instance of Sequential(). <br>\n",
    "The Sequential model is a linear stack of layers. We then keep adding Dense layers that are fully connected layers as we desire.<br><br>\n",
    "We have included Dropout using <i> nn_model.add(Dropout(0.3)) </i> <br><br>\n",
    "We can also include regularization using the command <br> <i> nn_model.add(Dense(21, activation='relu', kernel_regularizer=regularizers.l2(0.01))) </i> <br>instead of <br> <i> nn_model.add(Dense(21, activation='relu')) </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(35, input_dim=784, activation='relu'))\n",
    "nn_model.add(Dropout(0.3))\n",
    "nn_model.add(Dense(21, activation = 'relu'))\n",
    "nn_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the model on the training datasets, we compile the model in which we define various things like the loss function, the optimizer and the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to fit the model on the training input and training target dataset, we run the following command using a minibatch of size 10 and 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 10s 204us/step - loss: 0.5016 - acc: 0.8458\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 0.3222 - acc: 0.9018\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.2861 - acc: 0.9119\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 10s 196us/step - loss: 0.2657 - acc: 0.9172\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 10s 203us/step - loss: 0.2498 - acc: 0.9227\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.2467 - acc: 0.9225\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 10s 195us/step - loss: 0.2307 - acc: 0.9281\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 10s 191us/step - loss: 0.2283 - acc: 0.9286\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 9s 182us/step - loss: 0.2217 - acc: 0.9301\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 10s 202us/step - loss: 0.2195 - acc: 0.9305\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x202a97c4358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.fit(train_set_x, train_set_y, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 2s 30us/step\n",
      "\n",
      "acc: 96.89%\n"
     ]
    }
   ],
   "source": [
    "scores_train = nn_model.evaluate(train_set_x, train_set_y)\n",
    "print(\"\\n%s: %.2f%%\" % (nn_model.metrics_names[1], scores_train[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has ~ 97% accuracy on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 6, ..., 5, 6, 8], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = nn_model.predict(test_set_x)\n",
    "predictions = np.argmax(predictions, axis = 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 31us/step\n",
      "\n",
      "acc: 96.46%\n"
     ]
    }
   ],
   "source": [
    "scores_test = nn_model.evaluate(test_set_x, test_set_y)\n",
    "print(\"\\n%s: %.2f%%\" % (nn_model.metrics_names[1], scores_test[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has ~96% accuracy on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and look at the different test cases and check which all have gone wrong. Feel free to change the index numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-5325e5ce9d29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Label is {label}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "index  = 9997\n",
    "k = test_set_x[index, :]\n",
    "k = k.reshape((28, 28))\n",
    "plt.title('Label is {label}'.format(label=(predictions[index], np.argmax(test_set_y, axis = 1)[index])))\n",
    "plt.imshow(k, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
